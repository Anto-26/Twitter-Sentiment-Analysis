{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971ada30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a456a8a",
   "metadata": {},
   "source": [
    "Business Objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22f3c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis in business, also known as opinion mining is a process of identifying and cataloging a piece of text according to the tone conveyed by it. \n",
    "#This text can be tweets, comments, feedback, and even random rants with positive, negative and neutral sentiments associated with them. \n",
    "#Every business needs to implement automated sentiment analysis. \n",
    "#If you doubt it, here’s a little perspective. The accuracy can never be 100%. \n",
    "#And of course, a machine does not understand sarcasm. \n",
    "#However, according to a research, people do not agree 80% of the time. \n",
    "#It means that even if the machine accuracy does not score a perfect 10, it will still be more accurate than human analysis. \n",
    "#Also, when the corpus is huge, manually analyzing is not an option. Hence, sentiment analysis in business is more than just a trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e4f811",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The applications of sentiment analysis in business cannot be overlooked. \n",
    "#Sentiment analysis in business can prove a major breakthrough for the complete brand revitalization. \n",
    "#The key to running a successful business with the sentiments data is the ability to exploit the unstructured data for actionable insights. \n",
    "#Machine learning models, which largely depend on the manually created features before classification, have served this purpose fine for the past few years. \n",
    "#However, deep learning is a better choice as it:\n",
    "\n",
    "#Automatically extracts the relevant features.\n",
    "#Helps to scrape off the redundant features.\n",
    "#Rules out the efforts of manually crafting the features.\n",
    "#At ParallelDots, we have powerful sentiment analysis API that uses deep learning which provides an accurate analysis of the overall sentiment of the given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0122a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Dependencies\n",
    "\n",
    "# utilities\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "#from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# sklearn\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "#from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5528b2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dataset\n",
    "# The dataset being used is the sentiment140 dataset. It contains 1,600,000 tweets extracted using the Twitter API. The tweets \n",
    "# have been annotated (0 = Negative, 4 = Positive) and they can be used to detect sentiment.\n",
    "\n",
    "# Importing the dataset\n",
    "DATASET_COLUMNS  = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "dataset = pd.read_csv('Sentiment140.csv',\n",
    "                      encoding=DATASET_ENCODING , names=DATASET_COLUMNS)\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97fd5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a70d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It contains the following 6 fields:\n",
    "\n",
    "# 1.sentiment: the polarity of the tweet (0 = negative, 4 = positive)\n",
    "# 2.ids: The id of the tweet (2087)\n",
    "# 3.date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "# 4.flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "# 5.user: the user that tweeted (robotickilldozr)\n",
    "# 6.text: the text of the tweet (Lyx is cool)\n",
    "# We require only the sentiment and text fields, so we discard the rest.\n",
    "\n",
    "#Furthermore, we're changing the sentiment field so that it has new values to reflect the sentiment. (0 = Negative, 1 = Positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8f74e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the unnecessary columns.\n",
    "dataset = dataset[['sentiment','text']]\n",
    "# Replacing the values to ease understanding.\n",
    "dataset['sentiment'] = dataset['sentiment'].replace(4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ed5008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distribution for dataset.\n",
    "ax = dataset.groupby('sentiment').count().plot(kind='bar', title='Distribution of data',\n",
    "                                               legend=False)\n",
    "ax.set_xticklabels(['Negative','Positive'], rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdc2b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3254fde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing data in lists.\n",
    "text, sentiment = list(dataset['text']), list(dataset['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40578ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e4123e",
   "metadata": {},
   "source": [
    "Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e52363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, columns other than sentiment and target are redundant columns\n",
    "\n",
    "#redundant_dataset = dataset[['ids','date', 'flag', 'user']]\n",
    "#redundant_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc37650",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8114d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Text\n",
    "\n",
    "# Text Preprocessing is traditionally an important step for Natural Language Processing (NLP) tasks. It transforms text into a \n",
    "# more digestible form so that machine learning algorithms can perform better.\n",
    "\n",
    "# The Preprocessing steps taken are:\n",
    "\n",
    "# 1.Lower Casing: Each text is converted to lowercase.\n",
    "# 2.Replacing URLs: Links starting with \"http\" or \"https\" or \"www\" are replaced by \"URL\".\n",
    "# 3.Replacing Emojis: Replace emojis by using a pre-defined dictionary containing emojis along with their meaning. (eg: \":)\" to \"EMOJIsmile\")\n",
    "# 4.Replacing Usernames: Replace @Usernames with word \"USER\". (eg: \"@Kaggle\" to \"USER\")\n",
    "# 5.Removing Non-Alphabets: Replacing characters except Digits and Alphabets with a space.\n",
    "# 6.Removing Consecutive letters: 3 or more consecutive letters are replaced by 2 letters. (eg: \"Heyyyy\" to \"Heyy\")\n",
    "# 7.Removing Short Words: Words with length less than 2 are removed.\n",
    "# 8.Removing Stopwords: Stopwords are the English words which does not add much meaning to a sentence. They can safely be \n",
    "# ignored without sacrificing the meaning of the sentence. (eg: \"the\", \"he\", \"have\")\n",
    "# 9.Lemmatizing: Lemmatization is the process of converting a word to its base form. (e.g: “Great” to “Good”)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ba8df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining dictionary containing all emojis with their meanings.\n",
    "emojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n",
    "          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n",
    "          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n",
    "          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n",
    "          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n",
    "          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n",
    "          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b991ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining set containing all stopwords in english.\n",
    "stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
    "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
    "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
    "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from', \n",
    "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
    "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
    "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
    "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're',\n",
    "             's', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
    "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
    "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those', \n",
    "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
    "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
    "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
    "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc6bf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(textdata):\n",
    "    processedText = []\n",
    "    \n",
    "    # Create Lemmatizer and Stemmer.\n",
    "    wordLemm = WordNetLemmatizer()\n",
    "    \n",
    "    # Defining regex patterns.\n",
    "    urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "    userPattern       = '@[^\\s]+'\n",
    "    alphaPattern      = \"[^a-zA-Z0-9]\"\n",
    "    sequencePattern   = r\"(.)\\1\\1+\"\n",
    "    seqReplacePattern = r\"\\1\\1\"\n",
    "    \n",
    "    for tweet in textdata:\n",
    "        tweet = tweet.lower()\n",
    "        \n",
    "        # Replace all URls with 'URL'\n",
    "        tweet = re.sub(urlPattern,' URL',tweet)\n",
    "        # Replace all emojis.\n",
    "        for emoji in emojis.keys():\n",
    "            tweet = tweet.replace(emoji, \"EMOJI\" + emojis[emoji])        \n",
    "        # Replace @USERNAME to 'USER'.\n",
    "        tweet = re.sub(userPattern,' USER', tweet)        \n",
    "        # Replace all non alphabets.\n",
    "        tweet = re.sub(alphaPattern, \" \", tweet)\n",
    "        # Replace 3 or more consecutive letters by 2 letter.\n",
    "        tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n",
    "\n",
    "        tweetwords = ''\n",
    "        for word in tweet.split():\n",
    "            # Checking if the word is a stopword.\n",
    "            #if word not in stopwordlist:\n",
    "            if len(word)>1:\n",
    "                # Lemmatizing the word.\n",
    "                word = wordLemm.lemmatize(word)\n",
    "                tweetwords += (word+' ')\n",
    "                \n",
    "        processedText.append(tweetwords)\n",
    "        \n",
    "    return processedText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff34104",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "processedtext = preprocess(text)\n",
    "print(f'Text Preprocessing complete.')\n",
    "print(f'Time Taken: {round(time.time()-t)} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd28a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "processedtext[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44463eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysing the data\n",
    "\n",
    "# Now we're going to analyse the preprocessed data to get an understanding of it. We'll plot Word Clouds for Positive and \n",
    "# Negative tweets from our dataset and see which words occur the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f86d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word-Cloud for Negative tweets.\n",
    "\n",
    "data_neg = processedtext[:800000]\n",
    "plt.figure(figsize = (20,20))\n",
    "wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,\n",
    "               collocations=False).generate(\" \".join(data_neg))\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07bfe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word-Cloud for Positive tweets.\n",
    "\n",
    "#data_pos = processedtext[800000:]\n",
    "#plt.figure(figsize = (20,20))\n",
    "\n",
    "#wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,\n",
    " #             collocations=False).generate(\" \".join(data_pos))\n",
    "#plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b8ec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the Data\n",
    "# The Preprocessed Data is divided into 2 sets of data:\n",
    "\n",
    "# Training Data: The dataset upon which the model would be trained on. Contains 95% data.\n",
    "# Test Data: The dataset upon which the model would be tested against. Contains 5% data.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(processedtext[:1600000], sentiment,\n",
    "                                                    test_size = 0.05, random_state = 0)\n",
    "print(f'Data Split done.')\n",
    "# print('X_train',X_train.shape)\n",
    "# print('X_test',X_train.shape\n",
    "# print('Y_train',Y_train.shape)\n",
    "# print('X_test',Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281fac79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectoriser\n",
    "# TF-IDF indicates what the importance of the word is in order to understand the document or dataset. Let us understand with an\n",
    "# example. Suppose you have a dataset where students write an essay on the topic, My House. In this dataset, the word a appears\n",
    "# many times; it’s a high frequency word compared to other words in the dataset. The dataset contains other words like home, \n",
    "# house, rooms and so on that appear less often, so their frequency are lower and they carry more information compared to the \n",
    "# word. This is the intuition behind TF-IDF.\n",
    "\n",
    "# TF-IDF Vectoriser converts a collection of raw documents to a matrix of TF-IDF features. The Vectoriser is usually trained on \n",
    "# only the X_train dataset.\n",
    "\n",
    "# ngram_range is the range of number of words in a sequence. [e.g \"very expensive\" is a 2-gram that is considered as an extra \n",
    "# feature separately from \"very\" and \"expensive\" when you have a n-gram range of (1,2)]\n",
    "\n",
    "# max_features specifies the number of features to consider. [Ordered by feature frequency across the corpus]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8435e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\n",
    "vectoriser.fit(X_train)\n",
    "print(f'Vectoriser fitted.')\n",
    "print('No. of feature_words: ', len(vectoriser.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fb62b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranforming the dataset\n",
    "# Transforming the X_train and X_test dataset into matrix of TF-IDF Features by using the TF-IDF Vectoriser. This datasets will \n",
    "# be used to train the model and test against it.\n",
    "\n",
    "X_train = vectoriser.transform(X_train)\n",
    "X_test  = vectoriser.transform(X_test)\n",
    "print(f'Data Transformed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7bcb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and Evaluating Models\n",
    "# We're creating 3 different types of model for our sentiment analysis problem:\n",
    "\n",
    "# 1.Bernoulli Naive Bayes (BernoulliNB)\n",
    "# 2.Linear Support Vector Classification (LinearSVC)\n",
    "# 3.Logistic Regression (LR)\n",
    "\n",
    "# Since our dataset is not skewed, i.e. it has equal number of Positive and Negative Predictions. We're choosing Accuracy as our\n",
    "# evaluation metric. Furthermore, we're plotting the Confusion Matrix to get an understanding of how our model is performing on \n",
    "# both classification types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b852c516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model Function\n",
    "\n",
    "def model_Evaluate(model):\n",
    "    \n",
    "    # Predict values for Test dataset\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Print the evaluation metrics for the dataset.\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Compute and plot the Confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    categories  = ['Negative','Positive']\n",
    "    group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n",
    "\n",
    "    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "    sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',\n",
    "                xticklabels = categories, yticklabels = categories)\n",
    "\n",
    "    plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n",
    "    plt.ylabel(\"Actual values\"   , fontdict = {'size':14}, labelpad = 10)\n",
    "    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4998f38",
   "metadata": {},
   "source": [
    "# To predict Best Model and Over/Under fitting Identification accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f63673",
   "metadata": {},
   "source": [
    "# 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6579217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression Model\n",
    "LRmodel = LogisticRegression(C = 2, max_iter = 100, n_jobs=-1)\n",
    "LRmodel.fit(X_train, y_train)\n",
    "model_Evaluate(LRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edda492f",
   "metadata": {},
   "source": [
    "# 2. Bernoulli Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd1f119",
   "metadata": {},
   "outputs": [],
   "source": [
    "BNBmodel = BernoulliNB(alpha = 2)\n",
    "BNBmodel.fit(X_train, y_train)\n",
    "model_Evaluate(BNBmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809048be",
   "metadata": {},
   "source": [
    "# 3. Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bab1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=10)\n",
    "dt.fit(X_train, y_train)\n",
    "model_Evaluate(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5f8cf2",
   "metadata": {},
   "source": [
    "# 4.Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3196a605",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=20, max_depth=4, criterion='entropy', max_features=0.3, max_samples=0.7)\n",
    "rfc.fit(X_train, y_train)\n",
    "model_Evaluate(rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f58175",
   "metadata": {},
   "source": [
    "# 5.Ada Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b0e780",
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = AdaBoostClassifier()\n",
    "abc.fit(X_train, y_train)\n",
    "model_Evaluate(abc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b37f5e9",
   "metadata": {},
   "source": [
    "# 6.Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9111436",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier()\n",
    "gbc.fit(X_train, y_train)\n",
    "model_Evaluate(gbc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33e64cb",
   "metadata": {},
   "source": [
    "# 7.XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09978d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(val_metric='mlogloss')\n",
    "xgb.fit(X_train, y_train)\n",
    "model_Evaluate(xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49d15c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'USER no it not behaving at all mad why am here because can see you all over there ', # Actual 1 predicted 0\n",
    "'just re pierced my ear ' # Actual 1 predicted 0\n",
    "'USER would ve been the first but didn have gun not really though zac snyder just doucheclown ' # Actual 1 predicted 0\n",
    "'USER ahh ive always wanted to see rent love the soundtrack '# Actual 0 predicted 1\n",
    "'USER baked you cake but ated it ' # Actual 0 predicted 1\n",
    "'blagh class at tomorrow ' # Actual 1 predicted 0\n",
    "'ok sick and spent an hour sitting in the shower cause wa too sick to stand and held back the puke like champ bed now ' # Actual 0 predicted 1\n",
    "'USER sorry bed time came here gmt URL ' # Actual 0 predicted 1\n",
    "'he the reason for the teardrop on my guitar the only one who ha enough of me to break my heart ' # Actual 0 predicted 1\n",
    "'USER oh so sorry didn think about that before retweeting ' # Actual 0 predicted 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c839b6e9",
   "metadata": {},
   "source": [
    "# Saving the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f6fc53",
   "metadata": {},
   "source": [
    "We're using PICKLE to save Vectoriser and BernoulliNB, Logistic Regression Model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d89e443",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('vectoriser-ngram-(1,2).pickle','wb')\n",
    "pickle.dump(vectoriser, file)\n",
    "file.close()\n",
    "\n",
    "file = open('Sentiment-LR.pickle','wb')\n",
    "pickle.dump(LRmodel, file)\n",
    "file.close()\n",
    "\n",
    "file = open('Sentiment-BNB.pickle','wb')\n",
    "pickle.dump(BNBmodel, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4408897",
   "metadata": {},
   "source": [
    "Using the Model.\n",
    "To use the model for Sentiment Prediction we need to import the Vectoriser and LR Model using Pickle.\n",
    "\n",
    "The vectoriser can be used to transform data to matrix of TF-IDF Features. While the model can be used to predict the sentiment of the transformed Data. The text whose sentiment has to be predicted however must be preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeda15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    '''\n",
    "    Replace '..path/' by the path of the saved models.\n",
    "    '''\n",
    "    \n",
    "    # Load the vectoriser.\n",
    "    file = open('..path/vectoriser-ngram-(1,2).pickle', 'rb')\n",
    "    vectoriser = pickle.load(file)\n",
    "    file.close()\n",
    "    # Load the LR Model.\n",
    "    file = open('..path/Sentiment-LRv1.pickle', 'rb')\n",
    "    LRmodel = pickle.load(file)\n",
    "    file.close()\n",
    "    \n",
    "    return vectoriser, LRmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de9b135",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(vectoriser, model, text):\n",
    "    # Predict the sentiment\n",
    "    textdata = vectoriser.transform(preprocess(text))\n",
    "    sentiment = model.predict(textdata)\n",
    "    \n",
    "    # Make a list of text with sentiment.\n",
    "    data = []\n",
    "    for text, pred in zip(text, sentiment):\n",
    "        data.append((text,pred))\n",
    "        \n",
    "    # Convert the list into a Pandas DataFrame.\n",
    "    df = pd.DataFrame(data, columns = ['text','sentiment'])\n",
    "    df = df.replace([0,1], [\"Negative\",\"Positive\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f212b373",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    # Loading the models.\n",
    "    #vectoriser, LRmodel = load_models()\n",
    "    \n",
    "    # Text to classify should be in a list.\n",
    "    text = [\"I hate twitter\",\n",
    "            \"May the Force be with you.\",\n",
    "            \"Mr. Stark, I don't feel so good\"]\n",
    "    \n",
    "    df = predict(vectoriser, LRmodel, text)\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de9981f",
   "metadata": {},
   "source": [
    "We can clearly see that the Logistic Regression Model performs the best out of all the different models that we tried. It achieves nearly 82% accuracy while classifying the sentiment of a tweet.\n",
    "\n",
    "Although it should also be noted that the BernoulliNB Model is the fastest to train and predict on. It also achieves 80% accuracy while calssifying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955b19ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4b117a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ecf2d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
